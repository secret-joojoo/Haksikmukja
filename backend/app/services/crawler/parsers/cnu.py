from bs4 import BeautifulSoup
from datetime import date
from typing import List, Optional
import httpx
import re
from app.services.crawler.scrapers import BaseScraper
from app.schemas.crawler import SchoolData, CafeteriaData, MenuData

class CnuScraper(BaseScraper):
    # ì‹¤ì œ ì‹ë‹¹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì¸ë±ìŠ¤ 0ì€ ì œ1í•™ìƒíšŒê´€ì´ì§€ë§Œ, ë°ì´í„°ëŠ” 1ë²ˆë¶€í„° ì¡´ì¬)
    CAFETERIA_NAMES = ["ì œ1í•™ìƒíšŒê´€", "ì œ2í•™ìƒíšŒê´€", "ì œ3í•™ìƒíšŒê´€", "ì œ4í•™ìƒíšŒê´€", "ìƒí™œê³¼í•™ëŒ€í•™"]

    def __init__(self):
        super().__init__(
            school_name="ì¶©ë‚¨ëŒ€í•™êµ",
            school_region="ëŒ€ì „",
            url="https://mobileadmin.cnu.ac.kr/food/index.jsp"
        )

    async def fetch_html(self, target_date: date):
        formatted_date = target_date.strftime("%Y.%m.%d")
        target_url = f"{self.url}?searchYmd={formatted_date}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://mobileadmin.cnu.ac.kr/food/index.jsp"
        }

        async with httpx.AsyncClient(verify=False, headers=headers, timeout=20.0) as client:
            try:
                response = await client.get(target_url)
                return response.text if response.status_code == 200 else None
            except Exception as e:
                print(f"âŒ [ì¶©ë‚¨ëŒ€í•™êµ] ì ‘ì† ì—ëŸ¬: {e}")
                return None

    def _extract_menu_items(self, td) -> List[str]:
        if not td: return []
        p_tag = td.select_one("p")
        if not p_tag: return []
        
        text = p_tag.get_text(separator="\n", strip=True)
        items = [re.sub(r'\(.*\)', '', line).strip() for line in text.split("\n") if line.strip()]
        return [i for i in items if i and "ìš´ì˜ì•ˆí•¨" not in i and "ìš´ì˜ì¤‘ë‹¨" not in i]

    async def parse(self, target_date: date) -> Optional[SchoolData]:
        html = await self.fetch_html(target_date)
        if not html: return None

        soup = BeautifulSoup(html, 'html.parser')
        table = soup.select_one("table.menu-tbl")
        if not table: return None

        caf_data_map = {name: [] for name in self.CAFETERIA_NAMES}
        rows = table.select("tbody tr")
        
        # ğŸ’¡ [ì´ˆì—˜ë¦¬íŠ¸ ë¶„ì„ ê²°ê³¼] 
        # ì œ1í•™ìƒíšŒê´€ ì¹¸ì€ rowspan ë•Œë¬¸ì— ëª¨ë“  í–‰ì—ì„œ 'ì• ì¹¸'ì„ ì¡ì•„ë¨¹ê³  ìˆìŒ.
        # ë”°ë¼ì„œ ëª¨ë“  í–‰ì—ì„œ ì‹ë‹¹ ë°ì´í„°ëŠ” ì‹¤ì œ ì¸ë±ìŠ¤ë³´ë‹¤ 'ì•ìœ¼ë¡œ í•œ ì¹¸'ì”© ë‹¹ê²¨ì§.
        for idx, row in enumerate(rows):
            if idx in [0, 1]: meal_type = "BREAKFAST"
            elif idx in [2, 3]: meal_type = "LUNCH"
            elif idx in [4, 5]: meal_type = "DINNER"
            else: continue

            tds = row.find_all("td", recursive=False)
            
            # [ë³´ì • ë¡œì§]
            # 1. 'ì§ì›' í–‰(idx 0, 2, 4): [êµ¬ë¶„, ëŒ€ìƒ, (ì œ1ì€ ë³‘í•©ë¨), ì œ2, ì œ3, ì œ4, ìƒê³¼ëŒ€] ìˆœì„œ
            #    ì¦‰, tds[2]ê°€ ì œ2í•™ìƒíšŒê´€, tds[5]ê°€ ìƒí™œê³¼í•™ëŒ€í•™ì„.
            # 2. 'í•™ìƒ' í–‰(idx 1, 3, 5): [ëŒ€ìƒ, (ì œ1ì€ ë³‘í•©ë¨), ì œ2, ì œ3, ì œ4, ìƒê³¼ëŒ€] ìˆœì„œ
            #    ì¦‰, tds[1]ì´ ì œ2í•™ìƒíšŒê´€, tds[4]ê°€ ìƒí™œê³¼í•™ëŒ€í•™ì„.
            
            base_offset = 2 if idx % 2 == 0 else 1
            
            # ì œ2í•™ìƒíšŒê´€(index 1)ë¶€í„° ìƒí™œê³¼í•™ëŒ€í•™(index 4)ê¹Œì§€ ìˆ˜ì§‘
            for caf_idx in range(1, 5): 
                td_pos = base_offset + (caf_idx - 1)
                
                if td_pos < len(tds):
                    menu_items = self._extract_menu_items(tds[td_pos])
                    if menu_items:
                        caf_name = self.CAFETERIA_NAMES[caf_idx]
                        caf_data_map[caf_name].append(MenuData(
                            meal_type=meal_type,
                            menu_items=menu_items,
                            date=target_date
                        ))

        all_cafeterias = [
            CafeteriaData(name=name, menus=menus) 
            for name, menus in caf_data_map.items() if menus
        ]

        if not all_cafeterias: return None

        return SchoolData(
            school_name=self.school_name,
            school_region=self.school_region,
            cafeterias=all_cafeterias
        )